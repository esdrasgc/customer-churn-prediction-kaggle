{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#projeto-classificacao-redes-neurais-e-deep-learning-20252","title":"Projeto Classifica\u00e7\u00e3o - Redes neurais e deep learning 2025.2","text":"<p>O presente projeto trata da explora\u00e7\u00e3o do uso de redes neurais para atividades de predi\u00e7\u00e3o de dados. O objetivo foi buscar a explora\u00e7\u00e3o do MLP (Multi Layer Perceptron) para prever o churn de clientes de um desafio no Kaggle Ultimate Customer Churn Prediction Challenge.</p>"},{"location":"#grupo","title":"Grupo","text":"<ol> <li>Esdras Gomes Carvalho</li> <li>Lincoln Rodrigo Pereira de Melo</li> <li>Lucca d'Oliveira Gheti Kao</li> </ol> <p>Documenta\u00e7\u00e3o</p>"},{"location":"competition/","title":"Competition Submission","text":""},{"location":"competition/#submissao-para-a-competicao","title":"Submiss\u00e3o para a Competi\u00e7\u00e3o","text":"<p>Esta se\u00e7\u00e3o documenta a participa\u00e7\u00e3o na competi\u00e7\u00e3o Kaggle e os detalhes da submiss\u00e3o.</p>"},{"location":"competition/#informacoes-da-competicao","title":"Informa\u00e7\u00f5es da Competi\u00e7\u00e3o","text":"<ul> <li>Plataforma: Kaggle</li> <li>Nome: Ultimate Customer Churn Prediction Challenge</li> <li>Link: https://www.kaggle.com/competitions/ultimate-customer-churn-prediction-challenge</li> <li>M\u00e9trica Oficial: F1-score</li> <li>Tipo: Late Submission (competi\u00e7\u00e3o encerrada)</li> </ul>"},{"location":"competition/#estrutura-do-dataset","title":"Estrutura do Dataset","text":""},{"location":"competition/#dados-fornecidos","title":"Dados Fornecidos","text":"<ul> <li>train.csv: 8.000 amostras com target <code>Churn</code></li> <li>test.csv: 2.000 amostras sem target (para predi\u00e7\u00e3o)</li> <li>Colunas: 17 features + target (apenas em train)</li> </ul>"},{"location":"competition/#formato-de-submissao","title":"Formato de Submiss\u00e3o","text":"<pre><code>Customer_ID,Churn\n9001,0\n9002,1\n9003,0\n...\n</code></pre> <p>Requisitos: - Arquivo CSV com exatamente 2.000 linhas (+ header) - Coluna <code>Customer_ID</code> deve corresponder ao test.csv - Coluna <code>Churn</code> deve conter previs\u00f5es bin\u00e1rias (0 ou 1)</p>"},{"location":"competition/#pipeline-de-submissao","title":"Pipeline de Submiss\u00e3o","text":""},{"location":"competition/#1-pre-processamento-do-teste","title":"1. Pr\u00e9-processamento do Teste","text":"<pre><code>import pickle\nfrom sklearn.compose import ColumnTransformer\n\n# Carregar preprocessor treinado\nwith open('notebooks/artifacts/preprocessor.pkl', 'rb') as f:\n    preprocessor = pickle.load(f)\n\n# Aplicar no teste\nX_test = test[feature_vars]\nX_test_transformed = preprocessor.transform(X_test)\n</code></pre>"},{"location":"competition/#2-predicao","title":"2. Predi\u00e7\u00e3o","text":"<pre><code># Carregar modelo final (treinado em train+val)\nwith open('notebooks/artifacts/final_model_trainval.pkl', 'rb') as f:\n    model = pickle.load(f)\n\n# Predi\u00e7\u00f5es\ny_pred = model.predict(X_test_transformed)\n\n# OU, se quiser usar probabilidades com threshold customizado:\ny_proba = model.predict_proba(X_test_transformed)[:, 1]\ny_pred = (y_proba &gt;= 0.5).astype(int)  # threshold padr\u00e3o\n</code></pre>"},{"location":"competition/#3-gerar-submissao","title":"3. Gerar Submiss\u00e3o","text":"<pre><code>import pandas as pd\n\nsubmission = pd.DataFrame({\n    'Customer_ID': test['Customer_ID'],\n    'Churn': y_pred\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(f\"Submiss\u00e3o gerada: {len(submission)} linhas\")\n</code></pre>"},{"location":"competition/#detalhes-da-submissao","title":"Detalhes da Submiss\u00e3o","text":""},{"location":"competition/#threshold-de-classificacao","title":"Threshold de Classifica\u00e7\u00e3o","text":"<p>Threshold utilizado: 0.5 (padr\u00e3o)</p> <p>Justificativa:</p> <ul> <li>Split balanceado garante que probabilidades estejam centradas em 0.5</li> <li>Simplifica reprodu\u00e7\u00e3o</li> <li>N\u00e3o houve tempo para otimizar threshold no conjunto de valida\u00e7\u00e3o</li> </ul>"},{"location":"competition/#leaderboard","title":"Leaderboard","text":"<p>Estrutura:</p> <ul> <li>Public Leaderboard: 50% do test.csv (1.000 amostras)</li> <li>Private Leaderboard: 50% do test.csv (1.000 amostras) \u2014 usado para ranking final</li> </ul> <p>M\u00e9trica: F1-score</p> <p>Status da Submiss\u00e3o</p> <p>Nota: Esta se\u00e7\u00e3o ser\u00e1 atualizada ap\u00f3s a submiss\u00e3o oficial ser realizada.</p> <p>Para submeter: 1. Acessar p\u00e1gina da competi\u00e7\u00e3o 2. Fazer upload de <code>submission.csv</code> 3. Aguardar processamento 4. Verificar score no public leaderboard 5. Adicionar screenshot e link do perfil aqui</p>"},{"location":"competition/#bonus-da-disciplina","title":"B\u00f4nus da Disciplina","text":"<p>Conforme rubrica do projeto:</p> Pontos Descri\u00e7\u00e3o +0.5 Submiss\u00e3o v\u00e1lida em competi\u00e7\u00e3o reconhecida (proof required) +0.5 Submiss\u00e3o v\u00e1lida rankando no top 50% do leaderboard <p>Segue ent\u00e3o, o URL de submiss\u00e3o e do leaderboard da competi\u00e7\u00e3o.</p> <p> </p> Figura 1 \u2014 Submiss\u00e3o competi\u00e7\u00e3oFonte: Produ\u00e7\u00e3o Autoral <p> </p> Figura 2 \u2014 Ranking final da competi\u00e7\u00e3oFonte: Produ\u00e7\u00e3o Autoral"},{"location":"competition/#reprodutibilidade","title":"Reprodutibilidade","text":""},{"location":"competition/#modelo-utilizado","title":"Modelo Utilizado","text":"<ul> <li>Arquivo: <code>notebooks/artifacts/final_model_trainval.pkl</code></li> <li>Treinado em: train + validation (4.260 amostras)</li> <li>Hiperpar\u00e2metros: Conforme se\u00e7\u00e3o \"Implementa\u00e7\u00e3o do MLP\"</li> <li>Random state: 42</li> </ul>"},{"location":"conclusion/","title":"Conclusion","text":""},{"location":"conclusion/#conclusao","title":"Conclus\u00e3o","text":"<p>Este projeto aplicou MLPs (Multi-Layer Perceptrons) para predi\u00e7\u00e3o de churn em um dataset de competi\u00e7\u00e3o do Kaggle, explorando t\u00e9cnicas de pr\u00e9-processamento, otimiza\u00e7\u00e3o de hiperpar\u00e2metros e avalia\u00e7\u00e3o robusta.</p>"},{"location":"conclusion/#principais-resultados","title":"Principais Resultados","text":""},{"location":"conclusion/#performance-do-modelo","title":"Performance do Modelo","text":"<ul> <li>Acur\u00e1cia no teste: ~50.4%</li> <li>F1-score (classe 1): ~0.51</li> <li>Log Loss: ~0.693</li> </ul>"},{"location":"conclusion/#arquitetura-final","title":"Arquitetura Final","text":"<ul> <li>Camadas ocultas: [411, 359, 361] neur\u00f4nios</li> <li>Ativa\u00e7\u00e3o: logistic (sigmoid)</li> <li>Solver: LBFGS</li> <li>Regulariza\u00e7\u00e3o L2: \u03b1 = 7.78e-05</li> </ul>"},{"location":"conclusion/#principais-aprendizados","title":"Principais Aprendizados","text":""},{"location":"conclusion/#1-qualidade-dos-dados-e-crucial","title":"1. Qualidade dos Dados \u00e9 Crucial","text":"<p>A baixa correla\u00e7\u00e3o entre features e target (m\u00e1x ~0.02) limitou severamente a capacidade preditiva:</p> <ul> <li>Nenhuma vari\u00e1vel individual mostrou poder discriminante forte</li> <li>MLP n\u00e3o conseguiu capturar intera\u00e7\u00f5es n\u00e3o-lineares suficientemente \u00fateis</li> <li>Sugere que o dataset pode ser sinteticamente gerado sem padr\u00f5es reais</li> </ul>"},{"location":"conclusion/#2-balanceamento-de-classes","title":"2. Balanceamento de Classes","text":"<p>O split balanceado 70/15/15 foi essencial para:</p> <ul> <li>M\u00e9tricas est\u00e1veis e confi\u00e1veis</li> <li>Evitar vi\u00e9s do modelo para a classe majorit\u00e1ria</li> <li>Avaliar verdadeira capacidade de generaliza\u00e7\u00e3o</li> </ul>"},{"location":"conclusion/#3-otimizacao-de-hiperparametros","title":"3. Otimiza\u00e7\u00e3o de Hiperpar\u00e2metros","text":"<p>Optuna facilitou a busca autom\u00e1tica:</p> <ul> <li>10 trials foram suficientes para convergir</li> <li>LBFGS emergiu como solver mais robusto</li> <li>Regulariza\u00e7\u00e3o L2 preveniu overfitting</li> </ul>"},{"location":"conclusion/#4-importancia-do-pipeline-completo","title":"4. Import\u00e2ncia do Pipeline Completo","text":"<p>Implementar todo o fluxo (split \u2192 CV \u2192 treino \u2192 valida\u00e7\u00e3o \u2192 teste \u2192 retreino) ensinou:</p> <ul> <li>Nunca avaliar no conjunto de teste durante desenvolvimento</li> <li>Cross-validation fornece sinal de robustez</li> <li>Retreinar em train+val maximiza dados para submiss\u00e3o final</li> </ul>"},{"location":"conclusion/#limitacoes","title":"Limita\u00e7\u00f5es","text":""},{"location":"conclusion/#1-performance-do-modelo","title":"1. Performance do Modelo","text":"<ul> <li>Acur\u00e1cia de ~50% indica que o modelo n\u00e3o superou predi\u00e7\u00e3o aleat\u00f3ria</li> <li>Log loss ~0.693 sugere probabilidades mal calibradas</li> <li>F1-score baixo em ambas as classes</li> </ul>"},{"location":"conclusion/#2-falta-de-engenharia-de-features","title":"2. Falta de Engenharia de Features","text":"<ul> <li>N\u00e3o criamos features derivadas (intera\u00e7\u00f5es, agregados, raz\u00f5es)</li> <li>N\u00e3o exploramos conhecimento de dom\u00ednio espec\u00edfico de churn</li> <li>Usamos apenas as features fornecidas \"as-is\"</li> </ul>"},{"location":"conclusion/#3-modelos-alternativos-nao-explorados","title":"3. Modelos Alternativos N\u00e3o Explorados","text":"<ul> <li>N\u00e3o comparamos com tree-based models (XGBoost, LightGBM, CatBoost)</li> <li>N\u00e3o testamos ensembles ou stacking</li> <li>N\u00e3o exploramos redes mais profundas ou arquiteturas especializadas</li> </ul>"},{"location":"conclusion/#trabalhos-futuros","title":"Trabalhos Futuros","text":""},{"location":"conclusion/#melhoria-imediata","title":"Melhoria Imediata","text":"<ol> <li> <p>Feature Engineering:</p> </li> <li> <p>Criar raz\u00f5es: <code>Monthly_Spending / Account_Age_Months</code></p> </li> <li>Intera\u00e7\u00f5es: <code>Satisfaction_Score * Support_Calls</code></li> <li> <p>Binning de vari\u00e1veis num\u00e9ricas</p> </li> <li> <p>Modelos Alternativos:</p> </li> <li> <p>XGBoost/LightGBM (geralmente superiores em dados tabulares)</p> </li> <li>Ensemble de MLP + \u00e1rvores</li> <li> <p>Stacking com meta-learner</p> </li> <li> <p>Otimiza\u00e7\u00e3o de Threshold:</p> </li> <li> <p>Buscar threshold \u00f3timo para maximizar F1</p> </li> <li>Calibra\u00e7\u00e3o de probabilidades (Platt scaling, isotonic regression)</li> </ol>"},{"location":"conclusion/#exploracao-adicional","title":"Explora\u00e7\u00e3o Adicional","text":"<ol> <li> <p>An\u00e1lise de Erros:</p> </li> <li> <p>Investigar casos mal classificados</p> </li> <li>Identificar subgrupos problem\u00e1ticos</li> <li> <p>Criar features espec\u00edficas para esses casos</p> </li> <li> <p>Dados Externos:</p> </li> <li> <p>Buscar datasets de churn reais para valida\u00e7\u00e3o</p> </li> <li> <p>Comparar padr\u00f5es com literatura de churn</p> </li> <li> <p>T\u00e9cnicas Avan\u00e7adas:</p> </li> <li> <p>SMOTE/ADASYN para balanceamento sint\u00e9tico</p> </li> <li>Cost-sensitive learning</li> <li>Focal Loss para lidar com desbalanceamento</li> </ol>"},{"location":"conclusion/#consideracoes-finais","title":"Considera\u00e7\u00f5es Finais","text":"<p>Embora a performance absoluta tenha sido limitada, o projeto foi extremamente valioso para:</p> <ul> <li>Compreender o pipeline completo de ML</li> <li>Implementar split balanceado e cross-validation corretamente</li> <li>Usar Optuna para otimiza\u00e7\u00e3o autom\u00e1tica</li> <li>Praticar documenta\u00e7\u00e3o completa do projeto</li> <li>Reconhecer quando dados s\u00e3o o gargalo, n\u00e3o o modelo</li> </ul> <p>Em problemas reais de churn, espera-se F1-scores na faixa de 0.6-0.8 com bons dados e feature engineering adequado. Nosso resultado (~0.51) reflete as limita\u00e7\u00f5es do dataset fornecido.</p>"},{"location":"references/","title":"References","text":""},{"location":"references/#referencias","title":"Refer\u00eancias","text":""},{"location":"references/#dataset-e-competicao","title":"Dataset e Competi\u00e7\u00e3o","text":"<ul> <li> <p>Kaggle Competition: Ultimate Customer Churn Prediction Challenge https://www.kaggle.com/competitions/ultimate-customer-churn-prediction-challenge</p> </li> <li> <p>Citation:   Meziane, A. E. (2025). Ultimate Customer Churn Prediction Challenge. Kaggle.   https://kaggle.com/competitions/ultimate-customer-churn-prediction-challenge</p> </li> </ul>"},{"location":"references/#bibliotecas-e-frameworks","title":"Bibliotecas e Frameworks","text":""},{"location":"references/#scikit-learn","title":"Scikit-learn","text":"<ul> <li> <p>MLPClassifier Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html</p> </li> <li> <p>Model Evaluation: https://scikit-learn.org/stable/modules/model_evaluation.html</p> </li> <li> <p>ColumnTransformer: https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html</p> </li> <li> <p>Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825-2830.</p> </li> </ul>"},{"location":"references/#optuna","title":"Optuna","text":"<ul> <li> <p>Optuna Documentation: https://optuna.org/</p> </li> <li> <p>Akiba, T., et al. (2019). Optuna: A Next-generation Hyperparameter Optimization Framework. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 2623-2631.</p> </li> </ul>"},{"location":"references/#outras-bibliotecas","title":"Outras Bibliotecas","text":"<ul> <li>NumPy: https://numpy.org/</li> <li>Pandas: https://pandas.pydata.org/</li> <li>Polars: https://www.pola.rs/</li> <li>Matplotlib: https://matplotlib.org/</li> <li>Seaborn: https://seaborn.pydata.org/</li> </ul>"},{"location":"references/#documentacao","title":"Documenta\u00e7\u00e3o","text":"<ul> <li>MkDocs: https://www.mkdocs.org/</li> <li>MkDocs Material: https://squidfunk.github.io/mkdocs-material/</li> </ul>"},{"location":"references/#literatura-sobre-churn-prediction","title":"Literatura sobre Churn Prediction","text":"<ul> <li> <p>Verbeke, W., et al. (2012). New insights into churn prediction in the telecommunication sector: A profit driven data mining approach. European Journal of Operational Research, 218(1), 211-229.</p> </li> <li> <p>Hadden, J., et al. (2007). Computer assisted customer churn management: State-of-the-art and future trends. Computers &amp; Operations Research, 34(10), 2902-2917.</p> </li> <li> <p>Huang, B., Kechadi, M. T., &amp; Buckley, B. (2012). Customer churn prediction in telecommunications. Expert Systems with Applications, 39(1), 1414-1425.</p> </li> </ul>"},{"location":"references/#uso-de-ia","title":"Uso de IA","text":"<p>Declara\u00e7\u00e3o de Uso de IA</p> <p>Este projeto utilizou ferramentas de IA assistiva durante o desenvolvimento:</p> <ul> <li> <p>ChatGPT (OpenAI) e Claude (Anthropic): Auxiliaram na estrutura\u00e7\u00e3o inicial do c\u00f3digo, debug de erros, e gera\u00e7\u00e3o de snippets de c\u00f3digo para tarefas espec\u00edficas (e.g., implementa\u00e7\u00e3o de split balanceado, fun\u00e7\u00f5es de plotting).</p> </li> <li> <p>GitHub Copilot: Sugest\u00f5es de autocompletar durante codifica\u00e7\u00e3o de fun\u00e7\u00f5es repetitivas.</p> </li> </ul> <p>Nota importante: Todo o c\u00f3digo foi revisado, compreendido e adaptado pelos autores. Os autores s\u00e3o capazes de explicar qualquer parte do c\u00f3digo e an\u00e1lises apresentadas, conforme pol\u00edtica da disciplina.</p>"},{"location":"dataset/dataset_explanation/","title":"Dataset Explanation","text":""},{"location":"dataset/dataset_explanation/#dataset-explanation","title":"Dataset Explanation","text":"<p>A base de dados tabular, conforme j\u00e1 foi dito, conta com 8.000 clientes (linhas) e 17 colunas que descrevem perfil, plano/relacionamento e uso de um servi\u00e7o por assinatura, como por exemplo SaaS/telecom/streaming. Aqui ent\u00e3o, o objetivo \u00e9 prever churn \u2014 se o cliente cancela ou n\u00e3o o servi\u00e7o.</p> <ul> <li> <p><code>Churn</code> (inteiro bin\u00e1rio):</p> </li> <li> <p>0 = n\u00e3o cancelou</p> </li> <li>1 = cancelou</li> </ul>"},{"location":"dataset/dataset_explanation/#colunas-de-identificacao","title":"Colunas de identifica\u00e7\u00e3o","text":"<ul> <li><code>Customer_ID</code> (int): identificador \u00fanico do cliente.</li> </ul>"},{"location":"dataset/dataset_explanation/#features-inputs-e-tipos","title":"Features (inputs) e tipos","text":"<p>Categ\u00f3ricas (nominais)</p> <ul> <li><code>Gender</code> (objeto): {<code>Female</code>, <code>Male</code>}.</li> <li><code>Location</code> (objeto): {<code>California</code>, <code>Florida</code>, <code>Illinois</code>, <code>New York</code>, <code>Texas</code>}.</li> <li><code>Subscription_Type</code> (objeto): {<code>Basic</code>, <code>Premium</code>, <code>Enterprise</code>}.</li> <li><code>Last_Interaction_Type</code> (objeto): {<code>Negative</code>, <code>Neutral</code>, <code>Positive</code>}.</li> </ul> <p>Categ\u00f3rica bin\u00e1ria</p> <ul> <li><code>Promo_Opted_In</code> (int {0,1}): indica se aderiu a promo\u00e7\u00f5es.</li> </ul> <p>Num\u00e9ricas (cont\u00ednuas/discretas)</p> <ul> <li><code>Age</code> (int): idade; 18\u201369 (52 valores distintos).</li> <li><code>Account_Age_Months</code> (int): tempo de conta em meses; 1\u201359.</li> <li><code>Monthly_Spending</code> (float): gasto mensal; \u2248 10,09 a 199,94 (cont\u00ednua).</li> <li><code>Total_Usage_Hours</code> (int): horas totais de uso; 10\u2013499.</li> <li><code>Support_Calls</code> (int): n\u00famero de liga\u00e7\u00f5es ao suporte; 0\u20139.</li> <li><code>Late_Payments</code> (int): pagamentos em atraso; 0\u20134.</li> <li><code>Streaming_Usage</code> (int): uso (unidade de consumo de streaming); 0\u201399.</li> <li><code>Discount_Used</code> (int): descontos utilizados (unidade/\u00edndice); 0\u201399.</li> <li><code>Satisfaction_Score</code> (int, ordinal): escore de satisfa\u00e7\u00e3o; 1\u201310.</li> <li><code>Complaint_Tickets</code> (int): chamados de reclama\u00e7\u00e3o; 0\u20134.</li> </ul>"},{"location":"dataset/dataset_explanation/#distribuicao-dos-dados","title":"Distribui\u00e7\u00e3o dos dados","text":"<p>De modo geral, os dados n\u00e3o apresentam dados faltantes, provavelmente por ser uma competi\u00e7\u00e3o. Claro que a n\u00e3o exist\u00eancia de dados faltantes diminui a complexidade de prepara\u00e7\u00e3o dos dados, no entanto, no momento de previs\u00e3o a dificuldade n\u00e3o \u00e9 t\u00e3o impactada por isso.</p> <pre><code>&gt;&gt;&gt; df.to_pandas().info()\n\nRangeIndex: 8000 entries, 0 to 7999\nData columns (total 17 columns):\n\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Customer_ID            8000 non-null   int64  \n 1   Age                    8000 non-null   int64  \n 2   Gender                 8000 non-null   object \n 3   Location               8000 non-null   object \n 4   Subscription_Type      8000 non-null   object \n 5   Account_Age_Months     8000 non-null   int64  \n 6   Monthly_Spending       8000 non-null   float64\n 7   Total_Usage_Hours      8000 non-null   int64  \n 8   Support_Calls          8000 non-null   int64  \n 9   Late_Payments          8000 non-null   int64  \n 10  Streaming_Usage        8000 non-null   int64  \n 11  Discount_Used          8000 non-null   int64  \n 12  Satisfaction_Score     8000 non-null   int64  \n 13  Last_Interaction_Type  8000 non-null   object \n 14  Complaint_Tickets      8000 non-null   int64  \n 15  Promo_Opted_In         8000 non-null   int64  \n 16  Churn                  8000 non-null   int64  \n</code></pre> <p>Com rela\u00e7\u00e3o a distribui\u00e7\u00e3o dos dados, e esses perante as classes 0 e 1 da vari\u00e1vel Churn, \u00e9 poss\u00edvel observar que o conjunto de treinamento cont\u00e9m a maior parte das observa\u00e7\u00f5es sendo clientes ainda ativos, isto \u00e9, Churn = 0.</p> <p> </p> Figura 1 \u2014 Distribui\u00e7\u00e3o do target (Churn)Fonte: Produ\u00e7\u00e3o Autoral <p>Analisando agora as vari\u00e1veis categ\u00f3rias contra a vari\u00e1vel target (Churn), pode-se perceber que a distribui\u00e7\u00e3o das classes da vari\u00e1vel target cont\u00e9m o mesmo padr\u00e3o para a grande maioria das classes das vari\u00e1veis categ\u00f3ricas, sendo o mesmo da distribui\u00e7\u00e3o geral. Em outras palavras, o dataset apresenta \\(31,3%\\) das observa\u00e7\u00f5es sendo Churn = 1, e dentre todas as classes das vari\u00e1veis categ\u00f3rias (Gender, Location, Subscription_Type, Last_Interaction_Type), a classe Churn apresenta a mesma distribui\u00e7\u00e3o, ficando pr\u00f3ximo de 30% das observa\u00e7\u00f5es apresentadas. A vari\u00e1vel categ\u00f3ria que possui maior variabilidade relacionada a distribui\u00e7\u00e3o de Churn \u00e9 Location, que possui algumas classes como Florida e Illinois, com as distribui\u00e7\u00f5es de aproximadamente \\(33%\\) e \\(29%\\) respectivamente.</p> <p> </p> Figura 2 \u2014 Churn por vari\u00e1veis categ\u00f3ricasFonte: Produ\u00e7\u00e3o Autoral <p>Com rela\u00e7\u00e3o as vari\u00e1veis num\u00e9ricas, e distribui\u00e7\u00e3o das observa\u00e7\u00f5es, \u00e9 poss\u00edvel observar que a distin\u00e7\u00e3o entre um indiv\u00edduo Churn e outro N\u00e3o Churn, n\u00e3o afeta de forma relavante nenhuma vari\u00e1vel. \u00c9 poss\u00edvel identificar esse fato, observando onde existe os spikes na distribui\u00e7\u00e3o dos dados. Analisando os spikes entre a classe Churn e N\u00e3o Churn, \u00e9 dif\u00edcil identificar algum padr\u00e3o divergente entre eles. Ou seja, onde existe uma maior concentra\u00e7\u00e3o de observa\u00e7\u00f5es Churn, tamb\u00e9m existe uma maior concentra\u00e7\u00e3o de observa\u00e7\u00f5es N\u00e3o Churn, sempre nas suas devidas propor\u00e7\u00f5es em rela\u00e7\u00e3o as observa\u00e7\u00f5es em geral, uma vez que a distribui\u00e7\u00e3o de Churn \u00e9 \\(66,67\\)% (1) e \\(33,33\\)% (0).</p> <p>\u00c9 poss\u00edvel identificar essa distribui\u00e7\u00e3o muito parecida para ambas as classes a partir dos boxplots, onde os quartis s\u00e3o muito parecidos em todas as vari\u00e1veis. A \u00fanica vari\u00e1vel n\u00famerica que possui uma maior distin\u00e7\u00e3o de distribui\u00e7\u00e3o em rela\u00e7\u00e3o ao Churn \u00e9 Satisfaction Score, o que faz bastante sentido por possuir uma rela\u00e7\u00e3o instr\u00ednseca com a decis\u00e3o do consumidor de cancelar ou n\u00e3o a sua assinatura.</p> <p> </p> Figura 3 \u2014 Distribui\u00e7\u00f5es num\u00e9ricas por classeFonte: Produ\u00e7\u00e3o Autoral <p> </p> Figura 4 \u2014 Boxplots por ChurnFonte: Produ\u00e7\u00e3o Autoral <p>De forma a reafirmar o que os gr\u00e1fico anteriores confirmaram - n\u00e3o existe nenhuma vari\u00e1vel explicitamente impactante na decis\u00e3o do indiv\u00edduo cancelar ou n\u00e3o a sua assinatura - os valores de correla\u00e7\u00e3o encontrados entre as vari\u00e1veis e a vari\u00e1vel target s\u00e3o muito pequenos, com o maior de todos sendo contra Discount Used \\(0.0204\\).</p> <p> </p> Figura 5 \u2014 Matriz de correla\u00e7\u00e3oFonte: Produ\u00e7\u00e3o Autoral <pre><code>Top 10 correla\u00e7\u00f5es com Churn:\nChurn                 1.000000\nDiscount_Used         0.020400\nSatisfaction_Score    0.019819\nPromo_Opted_In        0.017182\nLate_Payments         0.016602\nAge                   0.016458\nSupport_Calls         0.012913\nComplaint_Tickets     0.010890\nMonthly_Spending      0.009202\n</code></pre> <p> </p> Figura 6 \u2014 Pairplot: vari\u00e1veis selecionadas \u00d7 ChurnFonte: Produ\u00e7\u00e3o Autoral <p>Por fim, observando a distribui\u00e7\u00e3o das vari\u00e1veis num\u00e9ricas entre elas mesmas \u00e9 poss\u00edvel observar que a vari\u00e1vel Age n\u00e3o exerce um papel determinante no comportamento de sa\u00edda, j\u00e1 que tanto Churn quanto N\u00e3o Churn est\u00e3o distribu\u00eddos de maneira semelhante ao longo das faixas et\u00e1rias.</p> <p>Por outro lado, o Monthly Spending mostra um contraste mais relevante: clientes com gastos mais altos tendem a permanecer, enquanto aqueles com consumo menor aparecem em maior propor\u00e7\u00e3o entre os Churn. Esse padr\u00e3o sugere que o n\u00edvel de engajamento financeiro com o servi\u00e7o est\u00e1 relacionado \u00e0 fidelidade.</p> <p>A vari\u00e1vel Satisfaction Score se destaca como um dos fatores mais discriminantes. Clientes que atribu\u00edram notas mais baixas est\u00e3o concentrados na classe de churn, ao passo que aqueles com n\u00edveis elevados de satisfa\u00e7\u00e3o aparecem predominantemente entre os que permanecem. Em complemento, as Support Calls refor\u00e7am esse comportamento: indiv\u00edduos que acionaram o atendimento com maior frequ\u00eancia tamb\u00e9m apresentam maior propens\u00e3o ao churn, enquanto os clientes que raramente buscaram suporte s\u00e3o em sua maioria fi\u00e9is.</p> <p>Por fim, os Late Payments tamb\u00e9m revelam uma associa\u00e7\u00e3o com a sa\u00edda. Clientes sem hist\u00f3rico de atraso tendem a se manter ativos, ao passo que os que acumularam inadimpl\u00eancias figuram de forma mais destacada na classe de churn. A an\u00e1lise conjunta das vari\u00e1veis mostra que, embora idade n\u00e3o seja um diferencial, aspectos relacionados \u00e0 satisfa\u00e7\u00e3o, relacionamento com o suporte e disciplina de pagamento s\u00e3o determinantes na probabilidade de desligamento, sendo o gasto mensal um fator que refor\u00e7a ou atenua esses efeitos.</p>"},{"location":"dataset/dataset_selection/","title":"Dataset Selection","text":""},{"location":"dataset/dataset_selection/#dataset-selection","title":"Dataset Selection","text":"<p>No momento de decis\u00e3o do dataset utilizado nesse projeto, foi considerado a busca do grupo por uma maior intera\u00e7\u00e3o com dados reais, que poderiam complexificar os desafios trazidos pelo dataset. Diante disso, foram explorados diversos datasets de desafios/competi\u00e7\u00f5es presentes no Kaggle. Ao analisar algumas competi\u00e7\u00f5es, foi escolhida a competi\u00e7\u00e3o Ultimate Customer Churn Prediction Challenge, por trazer um problema real do dia a dia de empresas do varejo - como lidar com o cancelamento de assinaturas dos clientes.</p> <p>Al\u00e9m disso, o churn \u00e9 um dos indicadores mais cr\u00edticos para o varejo moderno, especialmente em modelos de assinatura e relacionamento cont\u00ednuo com o cliente: ele corr\u00f3i receita recorrente, reduz o LTV (lifetime value) e impede o payback do CAC (custo de aquisi\u00e7\u00e3o), pressionando diretamente a rentabilidade. Em mercados de margem apertada e alta competi\u00e7\u00e3o digital, pequenas varia\u00e7\u00f5es na taxa de cancelamento geram grandes impactos no fluxo de caixa e no planejamento de demanda.</p> <p>Em s\u00edntese, entendendo a import\u00e2ncia desse indicador e por consequ\u00eancia a previs\u00e3o dele, o dataset foi selecionado, buscando replicar o desafio do dia a dia de diversas varejistas atrav\u00e9s da competi\u00e7\u00e3o.</p> <p>Com rela\u00e7\u00e3o ao dataset em si, ele conta com \\(17\\) colunas, sendo uma delas o target (Churn):</p> <ul> <li>Customer_ID</li> <li>Age</li> <li>Gender</li> <li>Location</li> <li>Subscription_Type</li> <li>Account_Age_Months</li> <li>Monthly_Spending</li> <li>Total_Usage_Hours</li> <li>Support_Calls</li> <li>Late_Payments</li> <li>Streaming_Usage</li> <li>Discount_Used</li> <li>Satisfaction_Score</li> <li>Last_Interaction_Type</li> <li>Complaint_Tickets</li> <li>Promo_Opted_In</li> <li>Churn</li> </ul> <p>Contando com \\(8000\\) linhas no sample de treino e \\(2000\\) linhas no sample de teste utilizado para submiss\u00e3o da competi\u00e7\u00e3o.</p>"},{"location":"evaluation/error_curves/","title":"Error Curves & Visualization","text":""},{"location":"evaluation/error_curves/#curvas-de-erro-e-visualizacao","title":"Curvas de Erro e Visualiza\u00e7\u00e3o","text":"<p>Esta se\u00e7\u00e3o apresenta as curvas de aprendizado e an\u00e1lise de converg\u00eancia do modelo.</p>"},{"location":"evaluation/error_curves/#curvas-de-treinamento","title":"Curvas de Treinamento","text":""},{"location":"evaluation/error_curves/#funcao-para-gerar-curvas","title":"Fun\u00e7\u00e3o para Gerar Curvas","text":"<p>Implementado em <code>train_mlp_cv_optuna.py</code>:</p> <pre><code>def plot_history(history: List[EpochMetrics]):\n    epochs = [m.epoch for m in history]\n    tr_loss = [m.train_loss for m in history]\n    va_loss = [m.val_loss for m in history]\n    tr_acc  = [m.train_acc  for m in history]\n    va_acc  = [m.val_acc    for m in history]\n\n    # Loss curves\n    plt.figure(figsize=(7, 5))\n    plt.plot(epochs, tr_loss, label=\"Train loss\")\n    plt.plot(epochs, va_loss, label=\"Val loss\")\n    plt.xlabel(\"\u00c9pocas\")\n    plt.ylabel(\"Log-loss\")\n    plt.title(\"Curva de perda por \u00e9poca\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    # Accuracy curves\n    plt.figure(figsize=(7, 5))\n    plt.plot(epochs, tr_acc, label=\"Train acc\")\n    plt.plot(epochs, va_acc, label=\"Val acc\")\n    plt.xlabel(\"\u00c9pocas\")\n    plt.ylabel(\"Acur\u00e1cia\")\n    plt.title(\"Acur\u00e1cia por \u00e9poca\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n</code></pre>"},{"location":"evaluation/error_curves/#observacoes-sobre-convergencia","title":"Observa\u00e7\u00f5es sobre Converg\u00eancia","text":""},{"location":"evaluation/error_curves/#modelo-final-lbfgs","title":"Modelo Final (LBFGS)","text":"<p>O solver LBFGS utilizado no modelo final:</p> <ul> <li>Converg\u00eancia r\u00e1pida (geralmente &lt; 100 itera\u00e7\u00f5es)</li> <li>N\u00e3o requer tuning manual de learning rate</li> <li>Otimizador de segunda ordem (usa informa\u00e7\u00e3o da Hessiana)</li> <li>Ideal para datasets de tamanho m\u00e9dio como o nosso (8k amostras)</li> </ul>"},{"location":"evaluation/error_curves/#treinamento-por-epoca-adamsgd","title":"Treinamento por \u00c9poca (Adam/SGD)","text":"<p>Quando usando <code>train_mlp_with_metrics()</code> com Adam:</p> <ul> <li>Curvas mais suaves e grad uais</li> <li>Early stopping previne overtraining</li> <li>Monitoramento de <code>val_f1</code> garante parada no ponto \u00f3timo</li> </ul>"},{"location":"evaluation/error_curves/#analise-de-threshold-f1-optimization","title":"An\u00e1lise de Threshold (F1 Optimization)","text":"<p>Para otimizar o F1-score (m\u00e9trica da competi\u00e7\u00e3o), pode-se variar o threshold de classifica\u00e7\u00e3o:</p> <pre><code>import numpy as np\nfrom sklearn.metrics import f1_score\n\n# Obter probabilidades\ny_val_proba = model.predict_proba(X_val)[:, 1]\n\n# Testar diferentes thresholds\nthresholds = np.linspace(0.3, 0.7, 41)\nf1_scores = []\n\nfor thresh in thresholds:\n    y_pred = (y_val_proba &gt;= thresh).astype(int)\n    f1 = f1_score(y_val, y_pred)\n    f1_scores.append(f1)\n\nbest_thresh = thresholds[np.argmax(f1_scores)]\nprint(f\"Melhor threshold: {best_thresh:.3f}\")\nprint(f\"F1-score m\u00e1ximo: {max(f1_scores):.4f}\")\n</code></pre>"},{"location":"evaluation/error_curves/#threshold-padrao","title":"Threshold Padr\u00e3o","text":"<p>Neste projeto, utilizamos threshold = 0.5 (padr\u00e3o), dado que:</p> <ul> <li>Classes foram balanceadas artificialmente nos splits</li> <li>Log loss j\u00e1 calibra bem as probabilidades</li> <li>Simplifica reprodu\u00e7\u00e3o dos resultados</li> </ul>"},{"location":"evaluation/metrics/","title":"Evaluation Metrics","text":""},{"location":"evaluation/metrics/#metricas-de-avaliacao","title":"M\u00e9tricas de Avalia\u00e7\u00e3o","text":"<p>Esta se\u00e7\u00e3o apresenta as m\u00e9tricas de desempenho do modelo no conjunto de teste.</p>"},{"location":"evaluation/metrics/#metricas-no-conjunto-de-teste","title":"M\u00e9tricas no Conjunto de Teste","text":"<p>Conforme <code>notebooks/3_train_egc.ipynb</code>, avaliamos o modelo em tr\u00eas conjuntos:</p>"},{"location":"evaluation/metrics/#modelo-treinado-apenas-em-train","title":"Modelo Treinado Apenas em TRAIN","text":"<p>Train Set (3.510 amostras): </p><pre><code>accuracy = 0.4929\nlogloss  = 0.693151\n\n              precision    recall  f1-score   support\n\n           0     0.4927    0.4826    0.4876      1755\n           1     0.4930    0.5031    0.4980      1755\n\n    accuracy                         0.4929      3510\n   macro avg     0.4929    0.4929    0.4928      3510\nweighted avg     0.4929    0.4929    0.4928      3510\n</code></pre><p></p> <p>Validation Set (750 amostras): </p><pre><code>accuracy = 0.5267\nlogloss  = 0.693145\n\n              precision    recall  f1-score   support\n\n           0     0.5278    0.5067    0.5170       375\n           1     0.5256    0.5467    0.5359       375\n\n    accuracy                         0.5267       750\n   macro avg     0.5267    0.5267    0.5265       750\nweighted avg     0.5267    0.5267    0.5265       750\n</code></pre><p></p> <p>Test Set (750 amostras): </p><pre><code>accuracy = 0.5067\nlogloss  = 0.693167\n\n              precision    recall  f1-score   support\n\n           0     0.5061    0.4987    0.5024       375\n           1     0.5073    0.5147    0.5109       375\n\n    accuracy                         0.5067       750\n   macro avg     0.5067    0.5067    0.5066       750\nweighted avg     0.5067    0.5067    0.5066       750\n</code></pre><p></p>"},{"location":"evaluation/metrics/#modelo-final-retreinado-em-trainval","title":"Modelo Final (Retreinado em TRAIN+VAL)","text":"<p>Ap\u00f3s retreinar em treino+valida\u00e7\u00e3o (4.260 amostras):</p> <p>Test Set (750 amostras): </p><pre><code>accuracy = 0.5040\nlogloss  = 0.693166\n\n              precision    recall  f1-score   support\n\n           0     0.5032    0.4960    0.4996       375\n           1     0.5048    0.5120    0.5084       375\n\n    accuracy                         0.5040       750\n   macro avg     0.5040    0.5040    0.5040       750\nweighted avg     0.5040    0.5040    0.5040       750\n</code></pre><p></p>"},{"location":"evaluation/metrics/#analise-das-metricas","title":"An\u00e1lise das M\u00e9tricas","text":""},{"location":"evaluation/metrics/#desempenho-geral","title":"Desempenho Geral","text":"<ul> <li>Acur\u00e1cia: ~50% (pr\u00f3ximo ao acaso para problema bin\u00e1rio)</li> <li>F1-score: ~0.50-0.51 (classe 1)</li> <li>Log Loss: ~0.693 (pr\u00f3ximo de ln(2) \u2248 0.693, indicador de probabilidades n\u00e3o calibradas)</li> </ul>"},{"location":"evaluation/metrics/#interpretacao","title":"Interpreta\u00e7\u00e3o","text":"<p>Performance Limitada</p> <p>Os resultados indicam que o modelo teve dificuldade em capturar padr\u00f5es discriminantes nos dados. Isso pode ser explicado por:</p> <ol> <li>Baixa correla\u00e7\u00e3o das features: como observado na EDA, nenhuma vari\u00e1vel individualmente apresenta correla\u00e7\u00e3o forte com Churn (m\u00e1x ~0.02)</li> <li>Dados simulados: dataset pode ter sido gerado sinteticamente com pouca estrutura real</li> <li>Complexidade do problema: churn \u00e9 inerentemente dif\u00edcil de prever com apenas dados transacionais</li> </ol>"},{"location":"evaluation/metrics/#comparacao-com-baseline","title":"Compara\u00e7\u00e3o com Baseline","text":"<p>Baseline (Majority Class):</p> <ul> <li>Prever sempre classe 0 (n\u00e3o churn): accuracy = 68.7%</li> <li>Prever sempre classe 1 (churn): accuracy = 31.3%</li> </ul> <p>Nosso modelo:</p> <ul> <li>Accuracy = 50.4% (pior que baseline!)</li> <li>Por\u00e9m, F1-score balanceado em ambas as classes</li> <li>Indica que o modelo est\u00e1 tentando generalizar, n\u00e3o apenas memorizar a classe majorit\u00e1ria</li> </ul>"},{"location":"evaluation/metrics/#visualizacoes-disponiveis","title":"Visualiza\u00e7\u00f5es Dispon\u00edveis","text":"<p>O notebook <code>3_train_egc.ipynb</code> gera automaticamente:</p> <ol> <li>Matriz de Confus\u00e3o (heatmap)</li> <li>Curva ROC com AUC</li> <li>Curva Precision-Recall com Average Precision</li> <li>Distribui\u00e7\u00e3o de probabilidades por classe</li> </ol> <p>Essas visualiza\u00e7\u00f5es s\u00e3o geradas via <code>plot_all()</code> em <code>train_mlp_cv_optuna.py</code>.</p>"},{"location":"evaluation/metrics/#metrica-da-competicao","title":"M\u00e9trica da Competi\u00e7\u00e3o","text":"<p>M\u00e9trica oficial: F1-score</p> <p>A competi\u00e7\u00e3o avalia com base no F1-score do conjunto de teste privado (50% das 2k amostras de teste).</p> <p>Nosso F1-score no teste interno: ~0.51 (classe 1)</p>"},{"location":"training/data_cleaning_normalization/","title":"Data Cleaning & Normalization","text":""},{"location":"training/data_cleaning_normalization/#limpeza-e-normalizacao-dos-dados","title":"Limpeza e Normaliza\u00e7\u00e3o dos Dados","text":"<p>Esta se\u00e7\u00e3o descreve como os dados foram preparados antes da modelagem.</p>"},{"location":"training/data_cleaning_normalization/#visao-geral","title":"Vis\u00e3o Geral","text":"<ul> <li>Classifica\u00e7\u00e3o bin\u00e1ria com vari\u00e1vel target <code>Churn</code> (0 = cliente ativo, 1 = cliente cancelou).</li> <li>Nenhum valor faltante detectado no arquivo de treinamento fornecido pela competi\u00e7\u00e3o.</li> <li>Implementamos pipeline robusto de pr\u00e9-processamento para generalizar melhor e suportar infer\u00eancia.</li> </ul>"},{"location":"training/data_cleaning_normalization/#pipeline-de-pre-processamento","title":"Pipeline de Pr\u00e9-processamento","text":"<p>Implementado em <code>notebooks/2_preprocess.ipynb</code> e <code>notebooks/train_mlp_cv_optuna.py</code> utilizando <code>ColumnTransformer</code> do scikit-learn:</p> <pre><code>from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_vars),\n        ('cat', OneHotEncoder(drop='first'), categorical_vars)\n    ],\n    remainder='passthrough'\n)\n</code></pre>"},{"location":"training/data_cleaning_normalization/#transformacoes-aplicadas","title":"Transforma\u00e7\u00f5es Aplicadas","text":"<ul> <li>Vari\u00e1veis Num\u00e9ricas: <code>StandardScaler()</code> para normaliza\u00e7\u00e3o (m\u00e9dia 0, desvio padr\u00e3o 1)</li> <li>Vari\u00e1veis Categ\u00f3ricas: <code>OneHotEncoder(drop='first')</code> para codifica\u00e7\u00e3o one-hot (evita dummy variable trap)</li> </ul>"},{"location":"training/data_cleaning_normalization/#tipagem-das-features","title":"Tipagem das Features","text":"<p>Conforme definido em <code>notebooks/2_preprocess.ipynb</code>:</p>"},{"location":"training/data_cleaning_normalization/#variaveis-categoricas","title":"Vari\u00e1veis Categ\u00f3ricas","text":"<ul> <li><code>Gender</code> (Masculino/Feminino)</li> <li><code>Location</code> (California, Florida, Illinois, New York, Texas)</li> <li><code>Subscription_Type</code> (Basic, Premium, Enterprise)</li> <li><code>Last_Interaction_Type</code> (Negative, Neutral, Positive)</li> <li><code>Promo_Opted_In</code> (0/1 \u2014 tratado como categ\u00f3rico no pipeline)</li> </ul>"},{"location":"training/data_cleaning_normalization/#variaveis-numericas-continuas","title":"Vari\u00e1veis Num\u00e9ricas Cont\u00ednuas","text":"<ul> <li><code>Age</code> (idade do cliente)</li> <li><code>Account_Age_Months</code> (tempo de conta em meses)</li> <li><code>Monthly_Spending</code> (gasto mensal)</li> <li><code>Total_Usage_Hours</code> (horas totais de uso)</li> <li><code>Streaming_Usage</code> (uso de streaming, 0-99%)</li> <li><code>Discount_Used</code> (desconto utilizado, 0-99%)</li> <li><code>Satisfaction_Score</code> (escore de satisfa\u00e7\u00e3o, 1-10)</li> </ul>"},{"location":"training/data_cleaning_normalization/#variaveis-numericas-discretas-contagens","title":"Vari\u00e1veis Num\u00e9ricas Discretas (Contagens)","text":"<ul> <li><code>Support_Calls</code> (n\u00famero de chamadas ao suporte)</li> <li><code>Late_Payments</code> (n\u00famero de pagamentos atrasados)</li> <li><code>Complaint_Tickets</code> (n\u00famero de tickets de reclama\u00e7\u00e3o)</li> </ul>"},{"location":"training/data_cleaning_normalization/#balanceamento-de-classes","title":"Balanceamento de Classes","text":"<ul> <li>O dataset de treino apresenta desbalanceamento: ~68,7% (classe 0) vs ~31,3% (classe 1).</li> <li>Implementamos split estratificado 70/15/15 balanceado via <code>stratified_custom_split_indices</code> em <code>notebooks/train_mlp_cv_optuna.py</code>.</li> <li>Cada split (treino/valida\u00e7\u00e3o/teste) cont\u00e9m o mesmo n\u00famero de positivos e negativos, baseado na classe minorit\u00e1ria.</li> </ul>"},{"location":"training/data_cleaning_normalization/#outliers-e-duplicados","title":"Outliers e Duplicados","text":"<ul> <li>Nenhuma remo\u00e7\u00e3o expl\u00edcita de outliers foi aplicada.</li> <li>Os dados parecem limpos/simulados para uso na competi\u00e7\u00e3o.</li> <li>Nenhuma duplicata foi reportada durante a EDA.</li> </ul>"},{"location":"training/data_cleaning_normalization/#reprodutibilidade","title":"Reprodutibilidade","text":"<ul> <li><code>random_state=42</code> fixado em todos os splits e inicializa\u00e7\u00f5es de modelo.</li> <li>Dados pr\u00e9-processados salvos em <code>data/X.pickle</code>, <code>data/y.pickle</code> e <code>data/X_teste.pickle</code>.</li> </ul>"},{"location":"training/mlp_implementation/","title":"MLP Implementation","text":""},{"location":"training/mlp_implementation/#implementacao-do-mlp","title":"Implementa\u00e7\u00e3o do MLP","text":"<p>Este projeto utiliza <code>MLPClassifier</code> do scikit-learn com otimiza\u00e7\u00e3o via Optuna, conforme implementado em <code>notebooks/3_train_egc.ipynb</code> e <code>notebooks/train_mlp_cv_optuna.py</code>.</p>"},{"location":"training/mlp_implementation/#implementacao-principal","title":"Implementa\u00e7\u00e3o Principal","text":""},{"location":"training/mlp_implementation/#modelo-base","title":"Modelo Base","text":"<ul> <li>Biblioteca: <code>sklearn.neural_network.MLPClassifier</code></li> <li>C\u00f3digo principal: <code>notebooks/train_mlp_cv_optuna.py</code></li> <li>Hiperpar\u00e2metros configur\u00e1veis:</li> <li><code>hidden_layer_sizes</code>: arquitetura das camadas ocultas</li> <li><code>activation</code>: {<code>relu</code>, <code>tanh</code>, <code>logistic</code> (sigmoid)}</li> <li><code>solver</code>: {<code>adam</code>, <code>lbfgs</code>, <code>sgd</code>}</li> <li><code>alpha</code>: regulariza\u00e7\u00e3o L2</li> <li><code>learning_rate_init</code>: taxa de aprendizado inicial</li> <li><code>random_state</code>: seed para reprodutibilidade</li> </ul>"},{"location":"training/mlp_implementation/#pre-processamento-integrado","title":"Pr\u00e9-processamento Integrado","text":"<p>Conforme <code>_build_preprocessor()</code> em <code>train_mlp_cv_optuna.py</code>: - Num\u00e9ricos: <code>SimpleImputer(median)</code> + <code>StandardScaler</code> - Categ\u00f3ricos: <code>SimpleImputer(most_frequent)</code> + <code>OneHotEncoder(handle_unknown='ignore')</code></p>"},{"location":"training/mlp_implementation/#otimizacao-de-hiperparametros-com-optuna","title":"Otimiza\u00e7\u00e3o de Hiperpar\u00e2metros com Optuna","text":"<p>Conforme <code>notebooks/3_train_egc.ipynb</code>, utilizamos Optuna para busca de hiperpar\u00e2metros:</p>"},{"location":"training/mlp_implementation/#espaco-de-busca","title":"Espa\u00e7o de Busca","text":"<pre><code>'n_layers': [1, 5]  # n\u00famero de camadas ocultas\n'layer_i_size': [50, 500]  # neur\u00f4nios por camada\n'activation': ['relu', 'tanh', 'logistic']\n'solver': ['adam', 'lbfgs']\n'alpha': [1e-6, 1e-1]  # log scale\n'learning_rate_init': [1e-5, 1e-1]  # log scale\n</code></pre>"},{"location":"training/mlp_implementation/#melhores-hiperparametros-encontrados","title":"Melhores Hiperpar\u00e2metros Encontrados","text":"<p>Ap\u00f3s 10 trials com otimiza\u00e7\u00e3o minimizando validation log loss:</p> <pre><code>{\n    'n_layers': 3,\n    'layer_0_size': 411,\n    'layer_1_size': 359,\n    'layer_2_size': 361,\n    'activation': 'logistic',\n    'solver': 'lbfgs',\n    'alpha': 7.78472202696525e-05,\n    'learning_rate_init': 3.2734483532096865e-05\n}\n</code></pre> <ul> <li>Arquitetura final: 3 camadas ocultas com [411, 359, 361] neur\u00f4nios</li> <li>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: <code>logistic</code> (sigmoid)</li> <li>Solver: <code>lbfgs</code> (otimizador de segunda ordem)</li> <li>Melhor val log loss: 0.693145</li> </ul>"},{"location":"training/mlp_implementation/#estrategia-de-treinamento","title":"Estrat\u00e9gia de Treinamento","text":""},{"location":"training/mlp_implementation/#split-balanceado","title":"Split Balanceado","text":"<pre><code>train shape: (3510, 20) class counts: Counter({1: 1755, 0: 1755})\nval   shape: (750, 20)  class counts: Counter({0: 375, 1: 375})\ntest  shape: (750, 20)  class counts: Counter({0: 375, 1: 375})\n</code></pre>"},{"location":"training/mlp_implementation/#pipeline-completo","title":"Pipeline Completo","text":"<ol> <li>Split estratificado 70/15/15 com balanceamento perfeito por classe</li> <li>Cross-validation (4 folds) no conjunto de treino para robustez</li> <li>Treinamento no conjunto completo de treino</li> <li>Avalia\u00e7\u00e3o em valida\u00e7\u00e3o e teste</li> <li>Re-treino final em treino+valida\u00e7\u00e3o para submiss\u00e3o</li> </ol>"},{"location":"training/mlp_implementation/#treinamento-por-epoca-alternativa","title":"Treinamento por \u00c9poca (Alternativa)","text":"<p>Em <code>train_mlp_cv_optuna.py</code>, implementamos tamb\u00e9m <code>train_mlp_with_metrics()</code> que:</p> <ul> <li>Usa <code>partial_fit</code> para treinamento iterativo por mini-batches</li> <li>Coleta m\u00e9tricas (loss, accuracy, F1) a cada \u00e9poca</li> <li>Implementa early stopping monitorando <code>val_f1</code> com <code>patience</code></li> <li>Restaura melhores pesos ao final</li> </ul>"},{"location":"training/model_training/","title":"Model Training","text":""},{"location":"training/model_training/#treinamento-do-modelo","title":"Treinamento do Modelo","text":"<p>Esta se\u00e7\u00e3o descreve o processo de treinamento do MLP e os desafios encontrados.</p>"},{"location":"training/model_training/#loop-de-treinamento","title":"Loop de Treinamento","text":""},{"location":"training/model_training/#abordagem-1-optuna-com-lbfgs-modelo-final","title":"Abordagem 1: Optuna com LBFGS (Modelo Final)","text":"<p>Conforme <code>notebooks/3_train_egc.ipynb</code>:</p> <pre><code>from train_mlp_cv_optuna import run_training\n\nresults = run_training(\n    X, y,\n    n_trials=10,\n    random_state=42,\n    cv_folds=4,\n    verbose=True\n)\n</code></pre> <p>Caracter\u00edsticas:</p> <ul> <li>Otimiza\u00e7\u00e3o autom\u00e1tica via Optuna (10 trials)</li> <li>Split balanceado 70/15/15 com classes equilibradas</li> <li>Cross-validation 4-fold no conjunto de treino</li> <li>M\u00e9trica de otimiza\u00e7\u00e3o: validation log loss (minimizar)</li> <li>Solver final: LBFGS (otimizador de segunda ordem, n\u00e3o requer tuning de learning rate)</li> </ul>"},{"location":"training/model_training/#abordagem-2-treinamento-por-epoca-com-early-stopping","title":"Abordagem 2: Treinamento por \u00c9poca com Early Stopping","text":"<p>Implementado em <code>train_mlp_with_metrics()</code> (<code>train_mlp_cv_optuna.py</code>):</p> <pre><code>clf, history, final_stats = train_mlp_with_metrics(\n    X_train, y_train, X_val, y_val,\n    hidden_layer_sizes=(64, 32),\n    activation=\"relu\",\n    solver=\"adam\",\n    alpha=1e-4,\n    learning_rate_init=1e-3,\n    batch_size=64,\n    max_epochs=200,\n    patience=20,\n    monitor=\"val_f1\",\n    monitor_mode=\"max\"\n)\n</code></pre> <p>M\u00e9tricas coletadas por \u00e9poca:</p> <ul> <li><code>train_loss</code>, <code>val_loss</code></li> <li><code>train_acc</code>, <code>val_acc</code></li> <li><code>train_f1</code>, <code>val_f1</code></li> </ul> <p>Early Stopping:</p> <ul> <li>Monitora <code>val_f1</code> (ou <code>val_loss</code>)</li> <li>Par\u00e2metro <code>patience</code>: n\u00famero de \u00e9pocas sem melhora antes de parar</li> <li>Restaura os melhores pesos ao final</li> </ul>"},{"location":"training/model_training/#inicializacao-e-regularizacao","title":"Inicializa\u00e7\u00e3o e Regulariza\u00e7\u00e3o","text":"<ul> <li>Inicializa\u00e7\u00e3o: <code>random_state=42</code> fixo para reprodutibilidade</li> <li>Regulariza\u00e7\u00e3o L2: <code>alpha</code> controla weight decay</li> <li>Valor \u00f3timo encontrado: <code>7.78e-05</code></li> </ul>"},{"location":"training/model_training/#artefatos-salvos","title":"Artefatos Salvos","text":"<p>Diret\u00f3rio: <code>notebooks/artifacts/</code></p> <ul> <li><code>best_model_train.pkl</code>: modelo treinado apenas no conjunto de treino</li> <li><code>final_model_trainval.pkl</code>: modelo retreinado em treino+valida\u00e7\u00e3o para submiss\u00e3o final</li> <li><code>optuna_study.pkl</code>: hist\u00f3rico completo da otimiza\u00e7\u00e3o Optuna</li> </ul>"},{"location":"training/model_training/#desafios-e-observacoes","title":"Desafios e Observa\u00e7\u00f5es","text":""},{"location":"training/model_training/#1-convergencia-do-modelo","title":"1. Converg\u00eancia do Modelo","text":"<ul> <li>LBFGS mostrou converg\u00eancia r\u00e1pida e est\u00e1vel</li> <li>Solvers como Adam requerem tuning cuidadoso de <code>learning_rate_init</code></li> <li>Modelos muito profundos (4-5 camadas) apresentaram maior dificuldade de converg\u00eancia</li> </ul>"},{"location":"training/model_training/#2-overfitting","title":"2. Overfitting","text":"<ul> <li>Dataset relativamente pequeno (8k amostras) exige regulariza\u00e7\u00e3o adequada</li> <li><code>alpha</code> (L2) foi crucial para evitar overfitting</li> <li>Early stopping baseado em <code>val_f1</code> preveniu treinamento excessivo</li> </ul>"},{"location":"training/model_training/#3-balanceamento-de-classes","title":"3. Balanceamento de Classes","text":"<ul> <li>Split balanceado 50/50 em cada parti\u00e7\u00e3o melhorou estabilidade das m\u00e9tricas</li> <li>Evita vi\u00e9s do modelo para a classe majorit\u00e1ria</li> </ul>"},{"location":"training/model_training/#reprodutibilidade","title":"Reprodutibilidade","text":"<ul> <li><code>random_state=42</code> usado consistentemente em:</li> <li>Splits de dados</li> <li>Inicializa\u00e7\u00e3o do modelo</li> <li>Cross-validation</li> <li>Permite reprodu\u00e7\u00e3o exata dos resultados</li> </ul>"},{"location":"training/training_testing_strategy/","title":"Training & Testing Strategy","text":""},{"location":"training/training_testing_strategy/#estrategia-de-treinamento-e-teste","title":"Estrat\u00e9gia de Treinamento e Teste","text":"<p>Esta se\u00e7\u00e3o detalha como os dados foram divididos e como garantimos avalia\u00e7\u00e3o robusta do modelo.</p>"},{"location":"training/training_testing_strategy/#divisao-dos-dados","title":"Divis\u00e3o dos Dados","text":""},{"location":"training/training_testing_strategy/#estrategia-de-split","title":"Estrat\u00e9gia de Split","text":"<p>Utilizamos split balanceado 70/15/15 implementado em <code>stratified_custom_split_indices()</code> (<code>train_mlp_cv_optuna.py</code>):</p> <pre><code>train_idx, val_idx, test_idx = stratified_custom_split_indices(\n    y, \n    random_state=42,\n    train_frac=0.70,\n    val_frac=0.15,\n    test_frac=0.15\n)\n</code></pre> <p>Caracter\u00edsticas:</p> <ul> <li>Cada parti\u00e7\u00e3o (treino/valida\u00e7\u00e3o/teste) cont\u00e9m exatamente o mesmo n\u00famero de positivos e negativos</li> <li>Baseado na classe minorit\u00e1ria para garantir balanceamento perfeito</li> <li>Garante <code>random_state=42</code> para reprodutibilidade</li> </ul>"},{"location":"training/training_testing_strategy/#tamanhos-dos-conjuntos","title":"Tamanhos dos Conjuntos","text":"<p>Conforme <code>notebooks/3_train_egc.ipynb</code>:</p> <pre><code>train shape: (3510, 20) class counts: {1: 1755, 0: 1755}\nval   shape: (750, 20)  class counts: {0: 375, 1: 375}\ntest  shape: (750, 20)  class counts: {0: 375, 1: 375}\n</code></pre> <ul> <li>Treino: 3.510 amostras (70% balanceado)</li> <li>Valida\u00e7\u00e3o: 750 amostras (15% balanceado)</li> <li>Teste: 750 amostras (15% balanceado)</li> </ul>"},{"location":"training/training_testing_strategy/#cross-validation","title":"Cross-Validation","text":""},{"location":"training/training_testing_strategy/#validacao-cruzada-no-treino","title":"Valida\u00e7\u00e3o Cruzada no Treino","text":"<p>Durante a otimiza\u00e7\u00e3o Optuna, aplicamos <code>StratifiedKFold</code> (4 folds) apenas no conjunto de treino:</p> <pre><code>cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\ncv_scores = cross_val_score(\n    pipe, X_train, y_train, \n    cv=cv, \n    scoring=\"accuracy\", \n    n_jobs=-1\n)\n</code></pre> <p>Objetivo:</p> <ul> <li>Sinal de robustez durante busca de hiperpar\u00e2metros</li> <li>Evita overfitting na sele\u00e7\u00e3o de modelo</li> <li>N\u00e3o \"contamina\" valida\u00e7\u00e3o/teste</li> </ul>"},{"location":"training/training_testing_strategy/#metricas-consideradas","title":"M\u00e9tricas Consideradas","text":"<ul> <li>Otimiza\u00e7\u00e3o prim\u00e1ria: <code>log_loss</code> (validation)</li> <li>M\u00e9tricas secund\u00e1rias: <code>accuracy</code>, <code>roc_auc</code>, <code>average_precision</code>, <code>f1</code></li> <li>M\u00e9trica da competi\u00e7\u00e3o: F1-score</li> </ul>"},{"location":"training/training_testing_strategy/#controle-de-overfitting","title":"Controle de Overfitting","text":""},{"location":"training/training_testing_strategy/#tecnicas-aplicadas","title":"T\u00e9cnicas Aplicadas","text":"<ol> <li>Regulariza\u00e7\u00e3o L2 (<code>alpha=7.78e-05</code>)</li> <li>Cross-validation durante busca de hiperpar\u00e2metros</li> <li>Split balanceado para m\u00e9tricas est\u00e1veis</li> <li>Early stopping (quando usando treinamento por \u00e9poca):</li> <li>Monitor: <code>val_f1</code> ou <code>val_loss</code></li> <li><code>patience</code>: n\u00famero de \u00e9pocas sem melhora</li> <li>Restaura melhores pesos</li> </ol>"},{"location":"training/training_testing_strategy/#pipeline-de-avaliacao","title":"Pipeline de Avalia\u00e7\u00e3o","text":""},{"location":"training/training_testing_strategy/#fase-1-desenvolvimento","title":"Fase 1: Desenvolvimento","text":"<ol> <li>Treinar no conjunto de treino (3.510 amostras)</li> <li>Selecionar hiperpar\u00e2metros usando valida\u00e7\u00e3o (750 amostras)</li> <li>Avaliar performance no teste (750 amostras)</li> </ol>"},{"location":"training/training_testing_strategy/#fase-2-submissao-final","title":"Fase 2: Submiss\u00e3o Final","text":"<ol> <li>Retreinar modelo com melhores hiperpar\u00e2metros em treino + valida\u00e7\u00e3o (4.260 amostras)</li> <li>Avaliar no teste para confirmar</li> <li>Aplicar no conjunto de teste da competi\u00e7\u00e3o (2.000 amostras)</li> </ol>"},{"location":"training/training_testing_strategy/#reprodutibilidade","title":"Reprodutibilidade","text":"<p>Seeds fixas em todos os componentes:</p> <ul> <li><code>random_state=42</code> em:</li> <li><code>stratified_custom_split_indices()</code></li> <li><code>MLPClassifier()</code></li> <li><code>StratifiedKFold()</code></li> <li><code>Optuna.Study(seed=42)</code> (quando aplic\u00e1vel)</li> </ul> <p>Dados salvos:</p> <ul> <li><code>data/X.pickle</code>, <code>data/y.pickle</code></li> <li><code>notebooks/artifacts/*.pkl</code></li> </ul>"}]}